---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vmoperator
  namespace: default
spec:
  chart:
    spec:
      chart: victoria-metrics-operator
      sourceRef:
        kind: HelmRepository
        name: vm
      version: 0.44.0
  interval: 1m0s
---
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMCluster
metadata:
  name: vmcluster
  namespace: monitoring
spec:
  retentionPeriod: "1"
  replicationFactor: 1

  vmstorage:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8482"
    replicaCount: 1
    image:
      repository: victoriametrics/vmstorage
      tag: v1.115.0-cluster
    volumeMounts:
      - name: vmstorage-data
        mountPath: /vmstorage-data
    volumes:
      - name: vmstorage-data
        persistentVolumeClaim:
          claimName: victoria-metrics
    initContainers:
      - name: cleanup-broken-parts
        image: alpine:3.19
        command:
          - /bin/sh
          - -c
          - |
            echo "üîç Scanning for corrupted metadata.json files..."
            apk add --no-cache jq findutils coreutils
            find /vmstorage-data/data -type f -name metadata.json | while read f; do
              if ! jq empty "$f" >/dev/null 2>&1; then
                echo "‚ö†Ô∏è Invalid JSON: $f"
                echo "üßπ Deleting corrupted part: $(dirname "$f")"
                rm -rf "$(dirname "$f")"
              fi
            done
            echo "‚úÖ Cleanup complete."
        volumeMounts:
          - name: vmstorage-data
            mountPath: /vmstorage-data

  vmselect:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8481"
    replicaCount: 1
    image:
      repository: victoriametrics/vmselect
      tag: v1.115.0-cluster

  vminsert:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8480"
    replicaCount: 1
    image:
      repository: victoriametrics/vminsert
      tag: v1.115.0-cluster
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vmagent
  namespace: default
spec:
  chart:
    spec:
      chart: victoria-metrics-agent
      sourceRef:
        kind: HelmRepository
        name: vm
      version: 0.18.0
  interval: 1m0s
  values:
    extraVolumes:
      - name: ha-bearer-token
        secret:
          secretName: home-assistant-token
    extraVolumeMounts:
      - name: ha-bearer-token
        mountPath: /etc/secrets/ha-bearer-token
        readOnly: true
    remoteWrite:
      - url: "http://vminsert-vmcluster.default.svc.cluster.local:8480/insert/0/prometheus/"
    config:
      global:
        scrape_interval: "10s"
      scrape_configs:
        - job_name: "hass"
          scrape_interval: "60s"
          metrics_path: "/api/prometheus"
          bearer_token_file: /etc/secrets/ha-bearer-token/token
          scheme: "http"
          static_configs:
            - targets:
                - "home-assistant:8123"
        - job_name: "vmagent"
          static_configs:
            - targets:
                - "localhost:8429"
        - job_name: "kubernetes-apiservers"
          kubernetes_sd_configs:
            - role: "endpoints"
          scheme: "https"
          tls_config:
            ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
            insecure_skip_verify: true
          bearer_token_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
          relabel_configs:
            - source_labels:
                - "__meta_kubernetes_namespace"
                - "__meta_kubernetes_service_name"
                - "__meta_kubernetes_endpoint_port_name"
              action: "keep"
              regex: "default;kubernetes;https"
        - job_name: "kubernetes-nodes"
          scheme: "https"
          tls_config:
            ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
            insecure_skip_verify: true
          bearer_token_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
          kubernetes_sd_configs:
            - role: "node"
          relabel_configs:
            - action: "labelmap"
              regex: "__meta_kubernetes_node_label_(.+)"
            - target_label: "__address__"
              replacement: "kubernetes.default.svc:443"
            - source_labels:
                - "__meta_kubernetes_node_name"
              regex: "(.+)"
              target_label: "__metrics_path__"
              replacement: "/api/v1/nodes/$1/proxy/metrics"
        - job_name: "kubernetes-nodes-cadvisor"
          scheme: "https"
          tls_config:
            ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
            insecure_skip_verify: true
          bearer_token_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
          kubernetes_sd_configs:
            - role: "node"
          relabel_configs:
            - action: "labelmap"
              regex: "__meta_kubernetes_node_label_(.+)"
            - target_label: "__address__"
              replacement: "kubernetes.default.svc:443"
            - source_labels:
                - "__meta_kubernetes_node_name"
              regex: "(.+)"
              target_label: "__metrics_path__"
              replacement: "/api/v1/nodes/$1/proxy/metrics/cadvisor"
          metric_relabel_configs:
            - action: "replace"
              source_labels:
                - "pod"
              regex: "(.+)"
              target_label: "pod_name"
              replacement: "${1}"
            - action: "replace"
              source_labels:
                - "container"
              regex: "(.+)"
              target_label: "container_name"
              replacement: "${1}"
            - action: "replace"
              target_label: "name"
              replacement: "k8s_stub"
            - action: "replace"
              source_labels:
                - "id"
              regex: "^/system\\.slice/(.+)\\.service$"
              target_label: "systemd_service_name"
              replacement: "${1}"